{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introducción a pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chanito/mlbourbaki/blob/main/Introducci%C3%B3n_a_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCbQIBhpmx_y"
      },
      "source": [
        "\n",
        "\n",
        "![image.png](https://spark.apache.org/docs/1.1.1/img/cluster-overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hi6OGDnO2f2"
      },
      "source": [
        "# Instalación de PySpark en Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BbA2UaqQOax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3b3c7d-b7ce-46c2-d085-85310d5960ed"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=aca00b7ae7e5b2994dd360c19e15fb00f50949372e01ee2af046b2308965e6ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf3kS3f2IcTW"
      },
      "source": [
        "# Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Mq8KI6xjd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "d83c632c-2a8a-4931-903c-d57097c22d90"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Prueba_spark_colab\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f31607cdb5a6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Prueba_spark_colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f688133f7d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxAujHu236V"
      },
      "source": [
        "# Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhBpsCzr296y"
      },
      "source": [
        "# Crear SparkContext para conectar con el cluster, antes es necesario tener SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[4]\"))\n",
        "\n",
        "# \"the master\" es la computadora conectada con el resto de las computadoras en el cluster que\n",
        "#  administra la división y transformación de los datos"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0b28MH2rE84"
      },
      "source": [
        "# Resilent Distributed Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFcA7v-rMAc"
      },
      "source": [
        "## Crear un objeto RDD paralelizando una colección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9osfnc07J7G1"
      },
      "source": [
        "a_np = np.random.randint(0,100,20)\n",
        "a_rdd = sc.parallelize(a_np)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx-Juh62JfEp",
        "outputId": "4ed76589-5dd7-44fa-9754-3f58ab23aa50"
      },
      "source": [
        "print(type(a_np))\n",
        "print(type(a_rdd))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'pyspark.rdd.RDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOPWDtGiJr7m",
        "outputId": "cb992f96-a0be-4382-88de-92b5a443ee51"
      },
      "source": [
        "a_rdd.collect()  # regresa los elementos distribuidos "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[61, 59, 58, 44, 21, 89, 64, 70, 6, 47, 17, 42, 97, 18, 56, 59, 58, 22, 80, 69]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7uz-hhTJzyl",
        "outputId": "303b4b22-5cf5-4d4d-8dd1-6f37f9d4c155"
      },
      "source": [
        "a_rdd.glom().collect()  # con glom podemos ver como se hicieron las particiones"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[61, 59, 58, 44, 21, 89, 64, 70, 6, 47],\n",
              " [17, 42, 97, 18, 56, 59, 58, 22, 80, 69]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5HK_MCeLIbe",
        "outputId": "161cd30b-7f7e-4589-b52d-3347038c5f97"
      },
      "source": [
        "sc.stop()\n",
        "sc=SparkContext(master=\"local[3]\")\n",
        "a_rdd = sc.parallelize(a_np)\n",
        "a_rdd.glom().collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[61, 59, 58, 44, 21, 89],\n",
              " [64, 70, 6, 47, 17, 42],\n",
              " [97, 18, 56, 59, 58, 22, 80, 69]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAnmLnZ9rauu"
      },
      "source": [
        "## Crear un objeto RDD a partir de datos externos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7h6Ff9rsvM"
      },
      "source": [
        "texto = sc.textFile(\"/content/sample_data/story of an hour.txt\")\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ieCOr2rtLMv",
        "outputId": "0d10ec73-cb49-427a-fe4a-f8fd8a3b08a5"
      },
      "source": [
        "type(texto)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s3PdaXItv90",
        "outputId": "94001a03-88cc-47ea-8f14-052052db374c"
      },
      "source": [
        "texto.map(lambda s: len(s)).reduce(lambda a, b: a + b) # cantidad de caracteres"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQhe7id2fwnG"
      },
      "source": [
        "# Operaciones en RDD\n",
        "- Transformaciones (Map)\n",
        "- Acciones (Reduce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOgVWsFCgwT3"
      },
      "source": [
        "### Transformaciones (Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9tqK1n3Is2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37d931c-525b-4d79-94d1-14c8b9e4fb62"
      },
      "source": [
        "#map function\n",
        "sc.parallelize([3,4,5]).map(lambda x: range(1,x)).collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[range(1, 3), range(1, 4), range(1, 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvtoyNXvf8Y3",
        "outputId": "bdbfef55-4fbb-4cc5-eaa4-2a844c9b49ae"
      },
      "source": [
        "#flatmap example.So it creates output like map function but it flattens the output in a list\n",
        "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVGAulI2louu",
        "outputId": "cbbd35c1-4aa1-4a37-e734-58ea08301bbf"
      },
      "source": [
        "#mapping con funciones regulares\n",
        "def square_if_odd(x):\n",
        "    \"\"\"\n",
        "    Si el numero es non, regresa el cuadrado, los pares en cambio\n",
        "    no sufren transformación\n",
        "    \"\"\"\n",
        "    if x%2==1:\n",
        "        return x*x\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "numeros = sc.parallelize(np.arange(20))\n",
        "numeros.map(square_if_odd).collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 9, 4, 25, 6, 49, 8, 81, 10, 121, 12, 169, 14, 225, 16, 289, 18, 361]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m27VEr7FgzKx"
      },
      "source": [
        "###  Acciones (Reduce)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsBpkk4thCgg"
      },
      "source": [
        "numbers = sc.parallelize([1, 4, 6, 2, 9, 10])\n",
        "sum = numbers.reduce(lambda a,b : a+b)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTdU9RrnLk13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530f80f4-5e69-4c8f-8bca-f50aefaf7e3d"
      },
      "source": [
        "numbers.count() #conotar los elementos"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NszV1--LoMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3088381-d6ae-4b76-f21e-b1091340c6e3"
      },
      "source": [
        "numbers.first()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws_K6B6ALrg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3067e628-0409-4fab-dca8-0217fc9cdf57"
      },
      "source": [
        "numbers.take(3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh3raTh6kIv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576a97cb-7dc8-4ae9-e138-d32713c9b28d"
      },
      "source": [
        "#Encontrar el elemento máximo con reduce\n",
        "numbers.reduce(lambda x,y: x if x > y else y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I5jLac6kx_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d1e431-e1c9-4f04-895e-96bdab6e769a"
      },
      "source": [
        "#Filtros: ejemplo devuelve numeros positivos divisibles entre 3\n",
        "numbers.filter(lambda x:x%3==0 and x>=0).collect()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XB1ReC1EkWkB",
        "outputId": "956b20e3-e222-4e76-9e70-8b72f6da28de"
      },
      "source": [
        "\n",
        "words = 'MapReduce, GFS, Hadoop, HDFS, Spark'.split(',')\n",
        "\n",
        "wordRDD = sc.parallelize(words)\n",
        "\n",
        "wordRDD.reduce(lambda w,v: w if len(w) >len(v) else v)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MapReduce'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KMr0uSvGlM2-",
        "outputId": "4a2f9a54-fe91-400b-9f9a-7fa10fe6277f"
      },
      "source": [
        "#uso de funciones comunes utilizando reduce \n",
        "def largerThan(x,y):\n",
        "    \"\"\"\n",
        "    Regresa la palabra más larga\n",
        "    \"\"\"\n",
        "    if len(x)> len(y):\n",
        "        return x\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "wordRDD.reduce(largerThan)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MapReduce'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QwVM2L5mGOW"
      },
      "source": [
        "# Groupby"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojdQX2vKme4W",
        "outputId": "6c651d0b-487f-425d-8bcb-709d5a64cb05"
      },
      "source": [
        "result=numeros.groupBy(lambda x:x%2).collect()\n",
        "result"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, <pyspark.resultiterable.ResultIterable at 0x7f68812a6b10>),\n",
              " (1, <pyspark.resultiterable.ResultIterable at 0x7f68812a6d50>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVS8q0GTmK2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8b7ae6-85d0-4369-f424-1b09ba60c2cf"
      },
      "source": [
        "sorted([(x, sorted(y)) for (x, y) in result])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]),\n",
              " (1, [1, 3, 5, 7, 9, 11, 13, 15, 17, 19])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeERnQF7nizZ"
      },
      "source": [
        "## Lazy Evaluation \n",
        "Es una estrategia de spark para acelerar operaciones paralelizadas.\n",
        "Deja lista una secuencia de tareas paso por paso en una tarea pero retraza la ejecución hasta que es absolutamente necesaria.\n",
        "\n",
        "(ejemplo en https://dzone.com/articles/the-benefits-amp-examples-of-using-apache-spark-wi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9u3wWRacEzu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Documentación oficial de spark: \n",
        "\n",
        "* [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#initializing-spark)\n",
        "\n",
        "* [Cluster Mode Oberview](https://databricks.com/blog/2018/05/03/benchmarking-apache-spark-on-a-single-node-machine.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6WzTp1zbgFA",
        "outputId": "dc2ae142-8f49-4760-e4b5-51f72b9a94de"
      },
      "source": [
        "!cat /proc/cpuinfo | grep processor | wc -l  # get number of processors in the vmachine"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}