{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regresión logística usando TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chanito/mlbourbaki/blob/main/Notebooks/Regresi%C3%B3n_log%C3%ADstica_usando_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-URr6ioO31Je"
      },
      "source": [
        "# Cargar Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXjUiWkiUQQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cb4b0d-72ac-482b-ca85-4c58d8ff69de"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "R_qNtUcq4MQN",
        "outputId": "1cd29398-0a10-4cb0-da47-b4a75194fe8a"
      },
      "source": [
        "datos = pd.read_csv('https://raw.githubusercontent.com/AnIsAsPe/LogisticRegression_SpamOpinion/master/Datos/deceptive-opinion.csv',\n",
        "                    usecols=['deceptive','text']\n",
        "                    )\n",
        "print(datos.shape)\n",
        "datos.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>deceptive</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>truthful</td>\n",
              "      <td>We stayed for a one night getaway with family ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>truthful</td>\n",
              "      <td>Triple A rate with upgrade to view room was le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>truthful</td>\n",
              "      <td>This comes a little late as I'm finally catchi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>truthful</td>\n",
              "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truthful</td>\n",
              "      <td>I asked for a high floor away from the elevato...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  deceptive                                               text\n",
              "0  truthful  We stayed for a one night getaway with family ...\n",
              "1  truthful  Triple A rate with upgrade to view room was le...\n",
              "2  truthful  This comes a little late as I'm finally catchi...\n",
              "3  truthful  The Omni Chicago really delivers on all fronts...\n",
              "4  truthful  I asked for a high floor away from the elevato..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEq5tt2N41nW",
        "outputId": "586c44bd-4c85-4c12-9fbb-7584da5c0ced"
      },
      "source": [
        "#datos['polarity'].value_counts()\n",
        "datos['deceptive'].value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "truthful     800\n",
              "deceptive    800\n",
              "Name: deceptive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcQidulQNd0d",
        "outputId": "f89cceb3-7777-4e88-a644-22a8c3ad2a8e"
      },
      "source": [
        "datos['deceptive'] = np.where(datos['deceptive']=='deceptive', 1, 0)\n",
        "datos['deceptive'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800\n",
              "0    800\n",
              "Name: deceptive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B10uYOdPAm2-"
      },
      "source": [
        "## Preprocesamiento de textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k_x2WD3-iZ6"
      },
      "source": [
        "def preprocesar(texto):\n",
        "  #convierte a minúsculas\n",
        "  texto = (texto).lower()\n",
        "\n",
        "  #elimina stopwords\n",
        "  stop = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "  texto = stop.sub('', texto) \n",
        "\n",
        "  #quita puntuaciones y números\n",
        "  texto = re.sub('[^a-z]+', ' ', texto)\n",
        "\n",
        "  #lematizar y quedarnos con palabras que tengan más de tres caracteres\n",
        "  st = PorterStemmer()\n",
        "  texto = texto.split()\n",
        "  texto = ' '.join([st.stem(i) for i in texto])\n",
        "  \n",
        "  return(texto)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "d5NaISfg6op2",
        "outputId": "43d5c80b-248e-4eae-f9bd-d12f25b560ee"
      },
      "source": [
        "datos['text_pp'] = datos['text'].apply(preprocesar)\n",
        "datos"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>deceptive</th>\n",
              "      <th>text</th>\n",
              "      <th>text_pp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>We stayed for a one night getaway with family ...</td>\n",
              "      <td>stay one night getaway famili thursday tripl a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Triple A rate with upgrade to view room was le...</td>\n",
              "      <td>tripl rate upgrad view room less also includ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>This comes a little late as I'm finally catchi...</td>\n",
              "      <td>come littl late final catch review past sever ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
              "      <td>omni chicago realli deliv front spacious room ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>I asked for a high floor away from the elevato...</td>\n",
              "      <td>ask high floor away elev got room pleasantli d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>1</td>\n",
              "      <td>Problems started when I booked the InterContin...</td>\n",
              "      <td>problem start book intercontinent chicago onli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>1</td>\n",
              "      <td>The Amalfi Hotel has a beautiful website and i...</td>\n",
              "      <td>amalfi hotel beauti websit interior decor wife...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>1</td>\n",
              "      <td>The Intercontinental Chicago Magnificent Mile ...</td>\n",
              "      <td>intercontinent chicago magnific mile outsid ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>1</td>\n",
              "      <td>The Palmer House Hilton, while it looks good i...</td>\n",
              "      <td>palmer hous hilton look good pictur outsid act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>1</td>\n",
              "      <td>As a former Chicagoan, I'm appalled at the Ama...</td>\n",
              "      <td>former chicagoan appal amalfi hotel chicago fi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      deceptive  ...                                            text_pp\n",
              "0             0  ...  stay one night getaway famili thursday tripl a...\n",
              "1             0  ...  tripl rate upgrad view room less also includ b...\n",
              "2             0  ...  come littl late final catch review past sever ...\n",
              "3             0  ...  omni chicago realli deliv front spacious room ...\n",
              "4             0  ...  ask high floor away elev got room pleasantli d...\n",
              "...         ...  ...                                                ...\n",
              "1595          1  ...  problem start book intercontinent chicago onli...\n",
              "1596          1  ...  amalfi hotel beauti websit interior decor wife...\n",
              "1597          1  ...  intercontinent chicago magnific mile outsid ho...\n",
              "1598          1  ...  palmer hous hilton look good pictur outsid act...\n",
              "1599          1  ...  former chicagoan appal amalfi hotel chicago fi...\n",
              "\n",
              "[1600 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihnhEfR9AsD1"
      },
      "source": [
        "# Vectorización de Texto mediante BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "034-OjI58ff2",
        "outputId": "f1a22afc-072a-4edd-cebc-70fe6e33c2f7"
      },
      "source": [
        "vectorizer = TfidfVectorizer(min_df=2)\n",
        "BOW = vectorizer.fit_transform(datos['text_pp'])\n",
        "BOW.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600, 3616)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_jB5UK8t_L8V",
        "outputId": "0d62f09f-97a5-4e33-b1e9-d46eca4a0f50"
      },
      "source": [
        "palabras = vectorizer.get_feature_names()\n",
        "pd.DataFrame(BOW.todense(), index=datos.index, columns=palabras)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aaa</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>abrupt</th>\n",
              "      <th>absenc</th>\n",
              "      <th>absent</th>\n",
              "      <th>absolut</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abund</th>\n",
              "      <th>abysm</th>\n",
              "      <th>ac</th>\n",
              "      <th>accent</th>\n",
              "      <th>accept</th>\n",
              "      <th>access</th>\n",
              "      <th>accident</th>\n",
              "      <th>accomad</th>\n",
              "      <th>accomid</th>\n",
              "      <th>accommod</th>\n",
              "      <th>accomod</th>\n",
              "      <th>accompani</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accord</th>\n",
              "      <th>account</th>\n",
              "      <th>accur</th>\n",
              "      <th>accus</th>\n",
              "      <th>accustom</th>\n",
              "      <th>ach</th>\n",
              "      <th>acknowledg</th>\n",
              "      <th>acquaint</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>activ</th>\n",
              "      <th>actual</th>\n",
              "      <th>ad</th>\n",
              "      <th>add</th>\n",
              "      <th>addit</th>\n",
              "      <th>address</th>\n",
              "      <th>adequ</th>\n",
              "      <th>adjac</th>\n",
              "      <th>...</th>\n",
              "      <th>worn</th>\n",
              "      <th>worri</th>\n",
              "      <th>wors</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthi</th>\n",
              "      <th>woudl</th>\n",
              "      <th>would</th>\n",
              "      <th>wouldnt</th>\n",
              "      <th>wow</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wrigley</th>\n",
              "      <th>wrinkl</th>\n",
              "      <th>write</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>ye</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearli</th>\n",
              "      <th>yell</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yelp</th>\n",
              "      <th>yep</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>yield</th>\n",
              "      <th>yoga</th>\n",
              "      <th>yogurt</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>yr</th>\n",
              "      <th>yuck</th>\n",
              "      <th>yummi</th>\n",
              "      <th>yup</th>\n",
              "      <th>zest</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1882</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111565</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.056084</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.304154</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.084973</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181761</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.096682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136929</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.128338</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111367</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.154996</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.162156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158274</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.082425</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 3616 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         aaa  abil       abl  abrupt  absenc  ...  yummi  yup  zest  zone  zoo\n",
              "0     0.1882   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1     0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "2     0.0000   0.0  0.084973     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "3     0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "4     0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "...      ...   ...       ...     ...     ...  ...    ...  ...   ...   ...  ...\n",
              "1595  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1596  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1597  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1598  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1599  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "\n",
              "[1600 rows x 3616 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcpeYCyCAws2"
      },
      "source": [
        "# Dividir conjunto de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhP0EnQSBEv0"
      },
      "source": [
        "X = BOW.todense()\n",
        "y = datos['deceptive']\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5cxknURA7oM"
      },
      "source": [
        "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                     random_state=3)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXPtxsn9DDmY",
        "outputId": "a530db41-636b-4338-e6d2-7c25261a392d"
      },
      "source": [
        "#Conjunto de entrenamiento\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1280, 3616), (1280,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m7lzaudDS4m",
        "outputId": "938cc70c-02a2-4ac7-df96-708b68cbfe49"
      },
      "source": [
        "#Conjunto de prueba\n",
        "X_test.shape, y_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((320, 3616), (320,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UHJqliwDqm3"
      },
      "source": [
        "# Diseño del modelo de red neuronal usando TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjgqDLdOUi1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc260ecf-a08a-4e93-b614-952cd395185e"
      },
      "source": [
        "import tensorflow as tf  \n",
        "print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPAq3gAuWX3Z"
      },
      "source": [
        "\n",
        "\n",
        "TensorFlow y Keras, ambos proyectos para el aprendizaje profundo,tienen una historia entrelazada. Keras era un conjunto de abstracciones que facilita el aprendizaje profundo, pero necesitada de un backend, desde la versión de Keras v1.1.0 TensorFlow fue el backend predeterminado (antes era Theano).\n",
        "\n",
        "A partir del lanzamiento de TensorFlow a mediados de 2019, Keras es ahora la API de alto nivel de TensorFlow para facilitar el diseño y entrenamiento de modelos rápidos y fáciles.  \n",
        "\n",
        "[Video sobre TensorFlow 2.0](https://www.youtube.com/watch?v=EqWsPO8DVXk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKxI5MTNQ29i",
        "outputId": "90dea933-d4d9-4fb8-caec-5811a6758852"
      },
      "source": [
        "model = tf.keras.Sequential([                     # https://www.tensorflow.org/guide/keras/sequential_model\n",
        "                             \n",
        "        tf.keras.Input(shape=(3616,)),              # dimensiones de entrada\n",
        "    \n",
        "        tf.keras.layers.Dense( \n",
        "                              1,                     # dimensiones de salida\n",
        "                              activation='sigmoid',  # función de activación  https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
        "                              name=\"layer_1\"         # nombre de la capa\n",
        "\n",
        "                              )\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKD8TCnERdjN"
      },
      "source": [
        "Una alternativa para establecer las dimensiones de entrada en la primera capa es utilizar el parámetro `input_shape`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSAJv-bJUoNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc5f86a-48b8-4779-9956-64471d379325"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    \n",
        "          tf.keras.layers.Dense(1, \n",
        "                                input_shape = (3616,),       # dimensiones de la entrada  \n",
        "                                activation='sigmoid',        # para la regresión logística\n",
        "                                name=\"layer_1\"               # nombre de la capa\n",
        "                                ),\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHjpA_I3P92N"
      },
      "source": [
        "Otra manera de construir un modelo secuencial es declrarlo y a continuación añadir capas ustilizando el método `add`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75thWRxDPzBh",
        "outputId": "2eda44ab-8c2a-4f38-cba3-bc9a6abd6271"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "    \n",
        "model.add(tf.keras.Input(shape=(3616,)))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1,\n",
        "                                  activation='sigmoid',        # para la regresión logística\n",
        "                                  name=\"layer_1\"   \n",
        "                                 ))\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAxKjodjP6Kk",
        "outputId": "f9028ab9-92a0-4816-df31-530665c6b8fd"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(1,\n",
        "                                  activation='sigmoid',       \n",
        "                                  name=\"layer_2\")  \n",
        "                                  )\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "_________________________________________________________________\n",
            "layer_2 (Dense)              (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 3,619\n",
            "Trainable params: 3,619\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcvcyUIKPYdl",
        "outputId": "d5d83d45-dee1-429b-d90d-62d7b8bde2a9"
      },
      "source": [
        "len(model.layers)  # layers es un atributo del modelo que regresa una lista con las capas del modelo"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "uCQHIuKbRp_r",
        "outputId": "125bb45a-670c-4dc1-f70a-63490e262c6c"
      },
      "source": [
        "tf.keras.utils.plot_model( \n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"LR\",\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJS9U_j2PVeU",
        "outputId": "2c4697b5-d2f8-402f-9c99-c02f39ff7862"
      },
      "source": [
        "\n",
        "model.pop()    # borra la última capa\n",
        "print(len(model.layers))\n",
        "model.summary()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEj4kqqjNfRW"
      },
      "source": [
        "# Entrenamiento y validación\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUXjspeTIoEE"
      },
      "source": [
        "model.compile(optimizer='adam',             # stochastic gradient descent adaptativo\n",
        "                                            # https://keras.io/api/optimizers/adam/\n",
        "              \n",
        "              loss='binary_crossentropy',   # función objetivo  que se busca minimizar\n",
        "                                            # https://keras.io/api/losses/\n",
        "              \n",
        "              metrics=['accuracy']\n",
        "              )\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9gjfbUKItmk",
        "outputId": "f88bc747-c5c8-4346-d075-cb11a33069a8"
      },
      "source": [
        "# model.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_test, y_test))\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.3408 - accuracy: 0.9719 - val_loss: 0.4808 - val_accuracy: 0.8500\n",
            "Epoch 2/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.9727 - val_loss: 0.4778 - val_accuracy: 0.8469\n",
            "Epoch 3/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.9727 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 4/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.9742 - val_loss: 0.4717 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.9734 - val_loss: 0.4686 - val_accuracy: 0.8531\n",
            "Epoch 6/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.9742 - val_loss: 0.4661 - val_accuracy: 0.8500\n",
            "Epoch 7/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.9742 - val_loss: 0.4639 - val_accuracy: 0.8438\n",
            "Epoch 8/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3056 - accuracy: 0.9750 - val_loss: 0.4606 - val_accuracy: 0.8531\n",
            "Epoch 9/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.3013 - accuracy: 0.9766 - val_loss: 0.4577 - val_accuracy: 0.8531\n",
            "Epoch 10/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2972 - accuracy: 0.9766 - val_loss: 0.4556 - val_accuracy: 0.8531\n",
            "Epoch 11/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.9766 - val_loss: 0.4532 - val_accuracy: 0.8531\n",
            "Epoch 12/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2893 - accuracy: 0.9766 - val_loss: 0.4510 - val_accuracy: 0.8562\n",
            "Epoch 13/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.9766 - val_loss: 0.4489 - val_accuracy: 0.8531\n",
            "Epoch 14/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.9789 - val_loss: 0.4466 - val_accuracy: 0.8531\n",
            "Epoch 15/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.9781 - val_loss: 0.4447 - val_accuracy: 0.8500\n",
            "Epoch 16/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.9789 - val_loss: 0.4425 - val_accuracy: 0.8469\n",
            "Epoch 17/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2715 - accuracy: 0.9781 - val_loss: 0.4408 - val_accuracy: 0.8469\n",
            "Epoch 18/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.9789 - val_loss: 0.4386 - val_accuracy: 0.8469\n",
            "Epoch 19/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.9797 - val_loss: 0.4368 - val_accuracy: 0.8469\n",
            "Epoch 20/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2618 - accuracy: 0.9789 - val_loss: 0.4350 - val_accuracy: 0.8469\n",
            "Epoch 21/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.9805 - val_loss: 0.4331 - val_accuracy: 0.8500\n",
            "Epoch 22/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.9797 - val_loss: 0.4317 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.9812 - val_loss: 0.4297 - val_accuracy: 0.8469\n",
            "Epoch 24/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.9812 - val_loss: 0.4281 - val_accuracy: 0.8531\n",
            "Epoch 25/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9812 - val_loss: 0.4266 - val_accuracy: 0.8500\n",
            "Epoch 26/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.9812 - val_loss: 0.4250 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.9812 - val_loss: 0.4235 - val_accuracy: 0.8562\n",
            "Epoch 28/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2392 - accuracy: 0.9820 - val_loss: 0.4214 - val_accuracy: 0.8562\n",
            "Epoch 29/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2367 - accuracy: 0.9812 - val_loss: 0.4208 - val_accuracy: 0.8500\n",
            "Epoch 30/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.9820 - val_loss: 0.4191 - val_accuracy: 0.8562\n",
            "Epoch 31/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.9828 - val_loss: 0.4176 - val_accuracy: 0.8531\n",
            "Epoch 32/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.9828 - val_loss: 0.4164 - val_accuracy: 0.8531\n",
            "Epoch 33/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.9828 - val_loss: 0.4149 - val_accuracy: 0.8562\n",
            "Epoch 34/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2244 - accuracy: 0.9828 - val_loss: 0.4136 - val_accuracy: 0.8562\n",
            "Epoch 35/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9828 - val_loss: 0.4120 - val_accuracy: 0.8562\n",
            "Epoch 36/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.9828 - val_loss: 0.4109 - val_accuracy: 0.8562\n",
            "Epoch 37/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9828 - val_loss: 0.4094 - val_accuracy: 0.8562\n",
            "Epoch 38/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9828 - val_loss: 0.4084 - val_accuracy: 0.8562\n",
            "Epoch 39/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9828 - val_loss: 0.4077 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9836 - val_loss: 0.4062 - val_accuracy: 0.8562\n",
            "Epoch 41/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9844 - val_loss: 0.4053 - val_accuracy: 0.8531\n",
            "Epoch 42/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9844 - val_loss: 0.4036 - val_accuracy: 0.8562\n",
            "Epoch 43/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9844 - val_loss: 0.4028 - val_accuracy: 0.8562\n",
            "Epoch 44/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9844 - val_loss: 0.4019 - val_accuracy: 0.8531\n",
            "Epoch 45/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9844 - val_loss: 0.4006 - val_accuracy: 0.8562\n",
            "Epoch 46/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9844 - val_loss: 0.3992 - val_accuracy: 0.8562\n",
            "Epoch 47/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.9844 - val_loss: 0.3980 - val_accuracy: 0.8562\n",
            "Epoch 48/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9844 - val_loss: 0.3973 - val_accuracy: 0.8562\n",
            "Epoch 49/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9844 - val_loss: 0.3964 - val_accuracy: 0.8562\n",
            "Epoch 50/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1916 - accuracy: 0.9844 - val_loss: 0.3955 - val_accuracy: 0.8562\n",
            "Epoch 51/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9844 - val_loss: 0.3945 - val_accuracy: 0.8562\n",
            "Epoch 52/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.9844 - val_loss: 0.3934 - val_accuracy: 0.8562\n",
            "Epoch 53/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9852 - val_loss: 0.3924 - val_accuracy: 0.8562\n",
            "Epoch 54/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9867 - val_loss: 0.3917 - val_accuracy: 0.8562\n",
            "Epoch 55/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.9867 - val_loss: 0.3906 - val_accuracy: 0.8562\n",
            "Epoch 56/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9875 - val_loss: 0.3899 - val_accuracy: 0.8562\n",
            "Epoch 57/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9875 - val_loss: 0.3890 - val_accuracy: 0.8562\n",
            "Epoch 58/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9883 - val_loss: 0.3877 - val_accuracy: 0.8594\n",
            "Epoch 59/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9883 - val_loss: 0.3872 - val_accuracy: 0.8562\n",
            "Epoch 60/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9898 - val_loss: 0.3862 - val_accuracy: 0.8594\n",
            "Epoch 61/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1732 - accuracy: 0.9898 - val_loss: 0.3857 - val_accuracy: 0.8562\n",
            "Epoch 62/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9906 - val_loss: 0.3849 - val_accuracy: 0.8594\n",
            "Epoch 63/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.9906 - val_loss: 0.3841 - val_accuracy: 0.8625\n",
            "Epoch 64/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.9906 - val_loss: 0.3832 - val_accuracy: 0.8625\n",
            "Epoch 65/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9906 - val_loss: 0.3824 - val_accuracy: 0.8625\n",
            "Epoch 66/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1656 - accuracy: 0.9906 - val_loss: 0.3817 - val_accuracy: 0.8625\n",
            "Epoch 67/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9906 - val_loss: 0.3812 - val_accuracy: 0.8625\n",
            "Epoch 68/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.9906 - val_loss: 0.3803 - val_accuracy: 0.8625\n",
            "Epoch 69/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1613 - accuracy: 0.9906 - val_loss: 0.3794 - val_accuracy: 0.8625\n",
            "Epoch 70/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9906 - val_loss: 0.3787 - val_accuracy: 0.8625\n",
            "Epoch 71/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.9906 - val_loss: 0.3781 - val_accuracy: 0.8625\n",
            "Epoch 72/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1571 - accuracy: 0.9906 - val_loss: 0.3773 - val_accuracy: 0.8625\n",
            "Epoch 73/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1558 - accuracy: 0.9914 - val_loss: 0.3768 - val_accuracy: 0.8625\n",
            "Epoch 74/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9914 - val_loss: 0.3762 - val_accuracy: 0.8625\n",
            "Epoch 75/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9914 - val_loss: 0.3753 - val_accuracy: 0.8625\n",
            "Epoch 76/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9914 - val_loss: 0.3748 - val_accuracy: 0.8625\n",
            "Epoch 77/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.9914 - val_loss: 0.3741 - val_accuracy: 0.8625\n",
            "Epoch 78/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1492 - accuracy: 0.9914 - val_loss: 0.3737 - val_accuracy: 0.8594\n",
            "Epoch 79/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1480 - accuracy: 0.9914 - val_loss: 0.3729 - val_accuracy: 0.8656\n",
            "Epoch 80/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9914 - val_loss: 0.3724 - val_accuracy: 0.8625\n",
            "Epoch 81/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9922 - val_loss: 0.3719 - val_accuracy: 0.8625\n",
            "Epoch 82/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9930 - val_loss: 0.3713 - val_accuracy: 0.8625\n",
            "Epoch 83/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9930 - val_loss: 0.3706 - val_accuracy: 0.8625\n",
            "Epoch 84/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9930 - val_loss: 0.3702 - val_accuracy: 0.8625\n",
            "Epoch 85/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9930 - val_loss: 0.3695 - val_accuracy: 0.8625\n",
            "Epoch 86/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9930 - val_loss: 0.3687 - val_accuracy: 0.8625\n",
            "Epoch 87/100\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.9930 - val_loss: 0.3684 - val_accuracy: 0.8625\n",
            "Epoch 88/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.9930 - val_loss: 0.3678 - val_accuracy: 0.8625\n",
            "Epoch 89/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9930 - val_loss: 0.3673 - val_accuracy: 0.8625\n",
            "Epoch 90/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9930 - val_loss: 0.3668 - val_accuracy: 0.8594\n",
            "Epoch 91/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1338 - accuracy: 0.9930 - val_loss: 0.3660 - val_accuracy: 0.8594\n",
            "Epoch 92/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9930 - val_loss: 0.3659 - val_accuracy: 0.8594\n",
            "Epoch 93/100\n",
            "40/40 [==============================] - 0s 4ms/step - loss: 0.1316 - accuracy: 0.9930 - val_loss: 0.3653 - val_accuracy: 0.8594\n",
            "Epoch 94/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9930 - val_loss: 0.3650 - val_accuracy: 0.8594\n",
            "Epoch 95/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9930 - val_loss: 0.3646 - val_accuracy: 0.8594\n",
            "Epoch 96/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1284 - accuracy: 0.9930 - val_loss: 0.3638 - val_accuracy: 0.8594\n",
            "Epoch 97/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9930 - val_loss: 0.3636 - val_accuracy: 0.8562\n",
            "Epoch 98/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1263 - accuracy: 0.9937 - val_loss: 0.3629 - val_accuracy: 0.8594\n",
            "Epoch 99/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9937 - val_loss: 0.3624 - val_accuracy: 0.8594\n",
            "Epoch 100/100\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9937 - val_loss: 0.3623 - val_accuracy: 0.8562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c154d7c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgfMP_nFUQs8"
      },
      "source": [
        "# Red neuronal de clasificación con más de una capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYY20FtbWiy",
        "outputId": "bd4d7359-1ac8-4942-874e-fffa0abbdb21"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "    \n",
        "model.add((tf.keras.layers.InputLayer(input_shape=(3616,))))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(12, activation='sigmoid',name=\"hidden_layer_1\" ))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(12, activation='sigmoid', name=\"hidden_layer_2\"))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1,activation='sigmoid', name=\"output_layer\" ))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer_1 (Dense)       (None, 12)                43404     \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 43,573\n",
            "Trainable params: 43,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "_gWCVSisk_cK",
        "outputId": "24af5568-beaf-41be-83b9-50cbd065b8fd"
      },
      "source": [
        "tf.keras.utils.plot_model( \n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"LR\",\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDdoPn6he4P-",
        "outputId": "07a0caca-7e29-45b9-d7ee-f116a1f70190"
      },
      "source": [
        "model.compile(optimizer='adam',             # stochastic gradient descent adaptativo\n",
        "                                            # https://keras.io/api/optimizers/adam/\n",
        "              \n",
        "              loss='binary_crossentropy',   # función objetivo  que se busca minimizar\n",
        "                                            # https://keras.io/api/losses/\n",
        "              \n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_test, y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 1s 16ms/step - loss: 0.7382 - accuracy: 0.5156 - val_loss: 0.7670 - val_accuracy: 0.4375\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.7134 - accuracy: 0.5156 - val_loss: 0.7372 - val_accuracy: 0.4375\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6988 - accuracy: 0.5156 - val_loss: 0.7160 - val_accuracy: 0.4375\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.6911 - accuracy: 0.5156 - val_loss: 0.7026 - val_accuracy: 0.4375\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.6870 - accuracy: 0.5156 - val_loss: 0.6959 - val_accuracy: 0.4375\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.6845 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.4375\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6825 - accuracy: 0.5219 - val_loss: 0.6894 - val_accuracy: 0.4437\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6805 - accuracy: 0.6078 - val_loss: 0.6871 - val_accuracy: 0.5063\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6782 - accuracy: 0.6047 - val_loss: 0.6867 - val_accuracy: 0.4688\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6757 - accuracy: 0.6617 - val_loss: 0.6846 - val_accuracy: 0.5375\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6730 - accuracy: 0.6812 - val_loss: 0.6837 - val_accuracy: 0.5063\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.6701 - accuracy: 0.6914 - val_loss: 0.6817 - val_accuracy: 0.5594\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6670 - accuracy: 0.8406 - val_loss: 0.6779 - val_accuracy: 0.7219\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.6631 - accuracy: 0.9000 - val_loss: 0.6761 - val_accuracy: 0.7125\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.6591 - accuracy: 0.8555 - val_loss: 0.6746 - val_accuracy: 0.6844\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6545 - accuracy: 0.8672 - val_loss: 0.6711 - val_accuracy: 0.7156\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6493 - accuracy: 0.9187 - val_loss: 0.6667 - val_accuracy: 0.7812\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6434 - accuracy: 0.9266 - val_loss: 0.6637 - val_accuracy: 0.7531\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.6368 - accuracy: 0.9297 - val_loss: 0.6589 - val_accuracy: 0.8000\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6294 - accuracy: 0.9352 - val_loss: 0.6543 - val_accuracy: 0.8062\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6211 - accuracy: 0.9461 - val_loss: 0.6483 - val_accuracy: 0.8219\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6119 - accuracy: 0.9469 - val_loss: 0.6426 - val_accuracy: 0.8250\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6016 - accuracy: 0.9516 - val_loss: 0.6363 - val_accuracy: 0.8188\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5904 - accuracy: 0.9539 - val_loss: 0.6280 - val_accuracy: 0.8344\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5780 - accuracy: 0.9563 - val_loss: 0.6200 - val_accuracy: 0.8438\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5646 - accuracy: 0.9555 - val_loss: 0.6121 - val_accuracy: 0.8344\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5504 - accuracy: 0.9547 - val_loss: 0.6035 - val_accuracy: 0.8281\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5350 - accuracy: 0.9563 - val_loss: 0.5928 - val_accuracy: 0.8406\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5187 - accuracy: 0.9578 - val_loss: 0.5830 - val_accuracy: 0.8375\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5018 - accuracy: 0.9633 - val_loss: 0.5711 - val_accuracy: 0.8500\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4841 - accuracy: 0.9625 - val_loss: 0.5603 - val_accuracy: 0.8500\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4658 - accuracy: 0.9633 - val_loss: 0.5492 - val_accuracy: 0.8469\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.9664 - val_loss: 0.5369 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4287 - accuracy: 0.9656 - val_loss: 0.5260 - val_accuracy: 0.8438\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4100 - accuracy: 0.9672 - val_loss: 0.5134 - val_accuracy: 0.8500\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.3915 - accuracy: 0.9688 - val_loss: 0.5023 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.3733 - accuracy: 0.9695 - val_loss: 0.4909 - val_accuracy: 0.8531\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.3556 - accuracy: 0.9703 - val_loss: 0.4798 - val_accuracy: 0.8531\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.3382 - accuracy: 0.9695 - val_loss: 0.4703 - val_accuracy: 0.8531\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.3215 - accuracy: 0.9719 - val_loss: 0.4603 - val_accuracy: 0.8531\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.3058 - accuracy: 0.9719 - val_loss: 0.4522 - val_accuracy: 0.8438\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.2904 - accuracy: 0.9758 - val_loss: 0.4407 - val_accuracy: 0.8531\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2758 - accuracy: 0.9773 - val_loss: 0.4336 - val_accuracy: 0.8531\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2620 - accuracy: 0.9773 - val_loss: 0.4255 - val_accuracy: 0.8531\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2489 - accuracy: 0.9773 - val_loss: 0.4189 - val_accuracy: 0.8500\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.2364 - accuracy: 0.9789 - val_loss: 0.4119 - val_accuracy: 0.8531\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.2249 - accuracy: 0.9797 - val_loss: 0.4041 - val_accuracy: 0.8531\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.2136 - accuracy: 0.9797 - val_loss: 0.3992 - val_accuracy: 0.8531\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.2032 - accuracy: 0.9805 - val_loss: 0.3953 - val_accuracy: 0.8438\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1933 - accuracy: 0.9820 - val_loss: 0.3894 - val_accuracy: 0.8469\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1839 - accuracy: 0.9844 - val_loss: 0.3844 - val_accuracy: 0.8500\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1753 - accuracy: 0.9852 - val_loss: 0.3791 - val_accuracy: 0.8531\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1669 - accuracy: 0.9852 - val_loss: 0.3764 - val_accuracy: 0.8500\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9852 - val_loss: 0.3747 - val_accuracy: 0.8469\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1516 - accuracy: 0.9852 - val_loss: 0.3706 - val_accuracy: 0.8469\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1445 - accuracy: 0.9867 - val_loss: 0.3669 - val_accuracy: 0.8469\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9875 - val_loss: 0.3644 - val_accuracy: 0.8469\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1317 - accuracy: 0.9891 - val_loss: 0.3623 - val_accuracy: 0.8469\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1257 - accuracy: 0.9898 - val_loss: 0.3606 - val_accuracy: 0.8469\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1201 - accuracy: 0.9898 - val_loss: 0.3592 - val_accuracy: 0.8406\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9906 - val_loss: 0.3571 - val_accuracy: 0.8406\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.1097 - accuracy: 0.9930 - val_loss: 0.3547 - val_accuracy: 0.8406\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1049 - accuracy: 0.9937 - val_loss: 0.3543 - val_accuracy: 0.8344\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.1004 - accuracy: 0.9937 - val_loss: 0.3521 - val_accuracy: 0.8406\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0960 - accuracy: 0.9937 - val_loss: 0.3516 - val_accuracy: 0.8375\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0919 - accuracy: 0.9953 - val_loss: 0.3514 - val_accuracy: 0.8375\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9953 - val_loss: 0.3502 - val_accuracy: 0.8375\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.9953 - val_loss: 0.3494 - val_accuracy: 0.8375\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9961 - val_loss: 0.3496 - val_accuracy: 0.8375\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9969 - val_loss: 0.3495 - val_accuracy: 0.8375\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0743 - accuracy: 0.9977 - val_loss: 0.3495 - val_accuracy: 0.8375\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0713 - accuracy: 0.9977 - val_loss: 0.3482 - val_accuracy: 0.8406\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0684 - accuracy: 0.9984 - val_loss: 0.3475 - val_accuracy: 0.8375\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.9992 - val_loss: 0.3478 - val_accuracy: 0.8375\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0631 - accuracy: 0.9992 - val_loss: 0.3482 - val_accuracy: 0.8344\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0607 - accuracy: 0.9992 - val_loss: 0.3484 - val_accuracy: 0.8375\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9992 - val_loss: 0.3488 - val_accuracy: 0.8313\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9992 - val_loss: 0.3487 - val_accuracy: 0.8313\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.9992 - val_loss: 0.3492 - val_accuracy: 0.8313\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0519 - accuracy: 0.9992 - val_loss: 0.3490 - val_accuracy: 0.8281\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.8281\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 0.3502 - val_accuracy: 0.8281\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 0.8281\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.8313\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 0.8250\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 0.8313\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.8313\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.8313\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.3549 - val_accuracy: 0.8281\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.8313\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 0.8313\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.8313\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.3574 - val_accuracy: 0.8313\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.8313\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 0.8313\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.3598 - val_accuracy: 0.8313\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.3597 - val_accuracy: 0.8313\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.8313\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 0.8313\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.8313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c12305ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RxZCrHva61q"
      },
      "source": [
        "# Referencia:\n",
        "\n",
        "1. Keras vs. tf.keras: What’s the difference in TensorFlow 2.0? [Internet]. PyImageSearch. 2019 [citado 20 de julio de 2021]. Disponible en: https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/\n"
      ]
    }
  ]
}